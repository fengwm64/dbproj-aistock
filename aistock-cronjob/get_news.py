# cronjob/get_hk_news.py
# çˆ¬å–æ¸¯ç¾è‚¡æ–°é—»å®šæ—¶ä»»åŠ¡ï¼Œé¡ºåºæ‰§è¡Œç‰ˆæœ¬

import os
import re
import time
import hashlib
from datetime import datetime
import requests
import openai
import numpy as np
from utils.save import save_news, initialize_database, save_news_embeddings, get_recent_news_with_embeddings, check_content_hashes, save_news_tags
from dotenv import load_dotenv
import sys
from bs4 import BeautifulSoup
from typing import Tuple, Optional, List, Dict
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)

# é…ç½®å‚æ•° 
CREATE_TITLE_MODEL = os.getenv("CREATE_TITLE_MODEL")
CREATE_TITLE_BASE_URL = os.getenv("CREATE_TITLE_BASE_URL")
CREATE_TITLE_API_KEY = os.getenv("CREATE_TITLE_API_KEY")

# åµŒå…¥APIé…ç½®
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL")
EMBEDDING_BASE_URL = os.getenv("EMBEDDING_BASE_URL")
EMBEDDING_API_KEY = os.getenv("EMBEDDING_API_KEY")

# å»é‡å‚æ•°
SIM_THRESHOLD = float(os.getenv("SIM_THRESHOLD", 0.9))
WINDOW_SIZE = int(os.getenv("WINDOW_SIZE", 5))

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
title_client = openai.OpenAI(api_key=CREATE_TITLE_API_KEY, base_url=CREATE_TITLE_BASE_URL)
embedding_client = openai.OpenAI(api_key=EMBEDDING_API_KEY, base_url=EMBEDDING_BASE_URL)

def generate_sign(ts: int, category: str) -> str:
    raw = f"app=CailianpressWeb&category={category}&lastTime={ts}&last_time={ts}&os=web&refresh_type=1&rn=20&sv=8.4.6"
    sha1_result = hashlib.sha1(raw.encode()).hexdigest()
    return hashlib.md5(sha1_result.encode()).hexdigest()

def format_ts(ts: int) -> str:
    return datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M:%S")

# ä½¿ç”¨APIè¿›è¡ŒåµŒå…¥å‡½æ•°
def create_embedding(content: str, model: str = None):
    """
    ä½¿ç”¨OpenAI APIç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
    """
    if model is None:
        model = EMBEDDING_MODEL
        
    try:
        # å¤„ç†æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¶…è¿‡æ¨¡å‹é™åˆ¶
        max_tokens = 8192  # è®¾ç½®ä¸€ä¸ªä¿å®ˆçš„é™åˆ¶ï¼Œä½äº8192çš„ä¸Šé™
        
        # æ–‡æœ¬è¿‡é•¿æ—¶è¿›è¡Œæˆªæ–­å¤„ç†
        if len(content) > max_tokens * 3:  # ç²—ç•¥ä¼°è®¡ï¼š1ä¸ªå­—ç¬¦çº¦å 0.33ä¸ªtoken           
            content = content[:max_tokens*2.8]  # æˆªæ–­è‡³çº¦2.5å€tokené™åˆ¶
            print(f"âš ï¸ æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³çº¦{len(content)}å­—ç¬¦")
        
        # ç›´æ¥è°ƒç”¨åŒæ­¥API
        response = embedding_client.embeddings.create(
            input=content,
            model=model
        )
        # è·å–åµŒå…¥å‘é‡
        embedding_vector = np.array(response.data[0].embedding)
        return embedding_vector
    except Exception as e:
        print(f"â—åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
        return None

# ä½¿ç”¨numpyè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
    """
    if a is None or b is None:
        return 0.0
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼šç‚¹ç§¯ / (å‘é‡açš„æ¨¡ * å‘é‡bçš„æ¨¡)
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    
    if norm_a == 0 or norm_b == 0:
        return 0.0
        
    return dot_product / (norm_a * norm_b)

def extract_news_link(url_or_schema):
    """ä»å„ç§æ ¼å¼çš„URLæˆ–schemaä¸­æå–æ–°é—»é“¾æ¥"""
    if not url_or_schema:
        return ""
    
    # å¤„ç†schemaæ ¼å¼
    if 'schema' in url_or_schema or '&id=' in url_or_schema or 'id=' in url_or_schema:
        # åŒ¹é…å½¢å¦‚ *id=xxx çš„æ¨¡å¼ï¼Œå…¶ä¸­*å¯ä»¥æ˜¯share_id, shareidç­‰
        match = re.search(r'(\w+id)=([A-Za-z0-9]+)', url_or_schema)
        if match:
            return f"https://www.cls.cn/detail/{match.group(2)}"
    
    # å¤„ç†æ™®é€šshareurl
    # åŒ¹é…å½¢å¦‚ /share/article/2040799 çš„æ¨¡å¼
    match = re.search(r'/share/article/(\d+)', url_or_schema)
    if match:
        return f"https://www.cls.cn/detail/{match.group(1)}"
    
    return url_or_schema  # å¦‚æœæ— æ³•æå–IDï¼Œåˆ™è¿”å›åŸå§‹URLæˆ–ç©ºå­—ç¬¦ä¸²

# æ·»åŠ åœ¨ extract_news_link å‡½æ•°ä¹‹å
def content_hash(text):
    """è®¡ç®—å†…å®¹å“ˆå¸Œå€¼"""
    return hashlib.sha256(text.encode('utf-8')).hexdigest()


# æ·»åŠ æ—¥å¿—æ§åˆ¶å‡½æ•°
def log_verbose(message, verbose_only=False):
    """æ§åˆ¶æ—¥å¿—è¾“å‡º"""
    # é»˜è®¤åªè¾“å‡ºé‡è¦ä¿¡æ¯ï¼Œè¯¦ç»†æ—¥å¿—å¯é€‰æ‹©æ€§å…³é—­
    if not verbose_only:
        print(message)

def crawl_content(url: str) -> Tuple[Optional[str], Optional[str]]:
    """çˆ¬å–æŒ‡å®šURLé¡µé¢ä¸­çš„æ‘˜è¦å’Œè¯¦ç»†å†…å®¹"""
    if not isinstance(url, str) or not url:
        return None, None
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8"
    }
    
    try:
        # å‡å°‘çˆ¬å–æ—¥å¿—ï¼Œæ”¹ä¸ºæ›´ç®€æ´çš„è¾“å‡º
        log_verbose(f"ğŸŒ çˆ¬å–: {url}", verbose_only=True)
        
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            log_verbose(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}", verbose_only=True)
            return None, None
            
        html_content = response.text
        soup = BeautifulSoup(html_content, "html.parser")
        
        # æ¨¡ç³ŠåŒ¹é…detail-brief
        brief_elem = None
        for elem in soup.find_all(class_=True):
            if "detail-brief" in ' '.join(elem.get('class', [])):
                brief_elem = elem
                break
        
        # å¤„ç†æ‘˜è¦å†…å®¹ - å»é™¤ã€ã€‘åŒ…è£¹çš„å†…å®¹
        brief_content = None
        if brief_elem:
            brief_text = brief_elem.get_text(strip=True)
            # å»é™¤ã€ã€‘åŒ…è£¹çš„å†…å®¹
            brief_content = re.sub(r'ã€[^ã€‘]*ã€‘', '', brief_text).strip()
        
        # æ¨¡ç³ŠåŒ¹é…detail-content
        content_elem = None
        for elem in soup.find_all(class_=True):
            if "detail-content" in ' '.join(elem.get('class', [])):
                content_elem = elem
                break
        
        # å¤„ç†è¯¦ç»†å†…å®¹
        content_text = None
        if content_elem:
            # å¤„ç†æ®µè½ï¼Œå¿½ç•¥<strong>å’Œ<a>æ ‡ç­¾ï¼Œä½†åœ¨æ¯ä¸ª<p>åæ·»åŠ æ¢è¡Œç¬¦
            paragraphs = []
            for p in content_elem.find_all('p'):
                # è·å–æ®µè½æ–‡æœ¬ï¼Œå¿½ç•¥å†…éƒ¨æ ‡ç­¾
                p_text = p.get_text(strip=True)
                # å»é™¤ã€ã€‘åŒ…è£¹çš„å†…å®¹
                p_text = re.sub(r'ã€[^ã€‘]*ã€‘', '', p_text).strip()
                if p_text:  # åªæ·»åŠ éç©ºæ®µè½
                    paragraphs.append(p_text)
            
            # ä½¿ç”¨æ¢è¡Œç¬¦è¿æ¥æ‰€æœ‰æ®µè½
            content_text = '\n'.join(paragraphs)
        
        # ç®€åŒ–æˆåŠŸæ—¥å¿—
        if brief_content or content_text:
            log_verbose(f"âœ… çˆ¬å–æˆåŠŸ: {url}", verbose_only=True)
            
        return brief_content, content_text

    except Exception as e:
        log_verbose(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}", verbose_only=True)
        return None, None

def create_title(content: str) -> tuple:
    """
    ä»å†…å®¹ä¸­æå–æ ‡é¢˜æˆ–ç”Ÿæˆæ–°æ ‡é¢˜ï¼ŒåŒæ—¶è¿”å›æ¸…ç†åçš„å†…å®¹
    
    å‚æ•°:
    - content: åŸå§‹å†…å®¹æ–‡æœ¬
    
    è¿”å›:
    - (title, cleaned_content): æ ‡é¢˜å’Œæ¸…ç†åçš„å†…å®¹
    """
    # æ£€æŸ¥å†…å®¹æ˜¯å¦å·²åŒ…å«ã€ã€‘æ ¼å¼çš„æ ‡é¢˜
    m = re.match(r"^ã€([^ã€‘]+)ã€‘(.*)", content)
    if m:
        title = m.group(1).strip()
        cleaned_content = m.group(2).strip()
        return title, cleaned_content
    
    # å¦‚æœæ²¡æœ‰ç°æˆæ ‡é¢˜ï¼Œéœ€è¦ç”Ÿæˆ
    cleaned_content = content.strip()
    
    # ä½¿ç”¨AIç”Ÿæˆæ ‡é¢˜
    prompt = f"ä½ æ˜¯ä¸€ä¸ªè´¢ç»æ–°é—»æ ‡é¢˜ç”Ÿæˆå™¨ï¼Œæˆ‘å°†ç»™ä½ ä¸€æ¡è´¢ç»æ–°é—»çš„å†…å®¹ï¼Œè¯·ä½ ç”Ÿæˆä¸€ä¸ªç®€æ´ã€æœ‰ä¿¡æ¯é‡çš„ä¸­æ–‡æ ‡é¢˜ï¼Œçªå‡ºé‡‘èè¦ç‚¹ï¼Œä¸è¶…è¿‡25ä¸ªæ±‰å­—ã€‚æ³¨æ„åªéœ€è¦è¾“å‡ºæ ‡é¢˜å†…å®¹ï¼Œæ— éœ€å…¶ä»–ä»»ä½•å¤šä½™ä¿¡æ¯ï¼Œä¸éœ€è¦ä½¿ç”¨ä»»ä½•ç¬¦å·åŒ…è£¹ã€‚ä»¥ä¸‹æ˜¯æ–°é—»å†…å®¹\n{cleaned_content}"
    try:
        # ç›´æ¥è°ƒç”¨åŒæ­¥API
        resp = title_client.chat.completions.create(
            model=CREATE_TITLE_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=64
        )
        title = resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"â—æ ‡é¢˜ç”Ÿæˆå¤±è´¥: {e}")
        traceback.print_exc()
        title = f"{cleaned_content[:20]}"
        
    return title, cleaned_content

def analyze_news_content(content, title=None, max_retries=3):
    """
    ä½¿ç”¨AIåˆ†ææ–°é—»å†…å®¹ï¼Œæå–æ ‡ç­¾å’Œæ‘˜è¦ï¼Œæ”¯æŒå¤±è´¥é‡è¯•
    
    å‚æ•°:
    - content: æ–°é—»å†…å®¹
    - title: å¯é€‰çš„æ–°é—»æ ‡é¢˜
    - max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
    - dict: åŒ…å«positive_tags, negative_tags, is_important, summaryçš„å­—å…¸
    """
    full_text = f"æ ‡é¢˜: {title}\nå†…å®¹: {content}" if title else content
    
    prompt = f"""ä½ ä¸€ä½é‡‘èæ–°é—»åˆ†æå¸ˆï¼Œè¯·ä½ åˆ†æä»¥ä¸‹è´¢ç»æ–°é—»å†…å®¹ï¼Œå¸®åŠ©æˆ‘æå–å…³é”®ä¿¡æ¯:
    
{full_text}

è¯·ä»¥JSONæ ¼å¼è¿”å›ä»¥ä¸‹ä¿¡æ¯:
1. positive_tags: è¿™æ¡æ–°é—»å¯¹å“ªäº›è‚¡ç¥¨æ¿å—/å“ªäº›è¡Œä¸šæ˜¯é‡å¤§åˆ©å¥½çš„ï¼Ÿæä¾›ä¸­æ–‡çš„æ ‡ç­¾åˆ—è¡¨ï¼Œæœ€å¤š3ä¸ªæ ‡ç­¾
2. negative_tags: è¿™æ¡æ–°é—»å¯¹å“ªäº›æ¿å—/å“ªäº›è¡Œä¸šæ˜¯é‡å¤§åˆ©ç©ºçš„ï¼Ÿæä¾›ä¸­æ–‡çš„æ ‡ç­¾åˆ—è¡¨ï¼Œæœ€å¤š3ä¸ªæ ‡ç­¾
3. is_important: è¿™æ˜¯å¦æ˜¯ä¸€æ¡é‡å¤§è´¢ç»äº‹ä»¶ï¼Ÿ(true/false)
4. summary: ç”¨50ä¸ªå­—ä»¥å†…æ€»ç»“è¿™æ¡æ–°é—»çš„è¦ç‚¹

å¿…é¡»ç›´æ¥è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼æ•°æ®ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜æˆ–ä»»ä½•å‰åç¼€ã€‚
æ— éœ€ä½¿ç”¨```json```åŒ…è£¹ï¼Œç›´æ¥è¿”å›JSON
positive_tagsä¸negative_tagsæ˜¯äº’æ–¥çš„ï¼Œå¦‚æœæœ‰positive_tagsé‚£negative_tagså°±åº”è¯¥ä¸ºç©º"""

    retries = 0
    while retries <= max_retries:
        try:
            # æ¯æ¬¡é‡è¯•æ—¶ç¨å¾®è°ƒæ•´æ¸©åº¦å‚æ•°
            temperature = 0.2 + (retries * 0.1)
            
            # ç›´æ¥è°ƒç”¨åŒæ­¥API
            resp = title_client.chat.completions.create(
                model=CREATE_TITLE_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=256,
                response_format={"type": "json_object"}
            )
            
            # è§£æJSONå“åº”
            result_text = resp.choices[0].message.content.strip()
            try:
                import json
                result = json.loads(result_text)
                # ç¡®ä¿æ‰€æœ‰å­—æ®µå­˜åœ¨
                return {
                    'positive_tags': result.get('positive_tags', []),
                    'negative_tags': result.get('negative_tags', []),
                    'is_important': result.get('is_important', False),
                    'summary': result.get('summary', '')
                }
            except json.JSONDecodeError:
                # å°è¯•ä»```json```æ ‡è®°ä¸­æå–JSON
                json_pattern = r'```json\s*([\s\S]*?)\s*```'
                match = re.search(json_pattern, result_text)
                if match:
                    try:
                        # æå–JSONå­—ç¬¦ä¸²å¹¶è§£æ
                        json_str = match.group(1).strip()
                        print(f"ğŸ” ä»ä»£ç å—ä¸­æå–JSON: {json_str[:100]}...")
                        result = json.loads(json_str)
                        # ç¡®ä¿æ‰€æœ‰å­—æ®µå­˜åœ¨
                        return {
                            'positive_tags': result.get('positive_tags', []),
                            'negative_tags': result.get('negative_tags', []),
                            'is_important': result.get('is_important', False),
                            'summary': result.get('summary', '')
                        }
                    except json.JSONDecodeError:
                        print("â— ä»ä»£ç å—æå–çš„JSONä»ç„¶æ— æ•ˆ")
                
                if retries < max_retries:
                    retries += 1
                    print(f"â— AIè¿”å›çš„JSONæ ¼å¼æ— æ•ˆ (ç¬¬{retries}æ¬¡å°è¯•)ï¼Œè¿›è¡Œé‡è¯•...")
                    print(result_text)
                    # å¢åŠ æç¤ºçš„æ˜ç¡®æ€§
                    prompt += "\n\nè¯·ç¡®ä¿è¿”å›æ ¼å¼æ­£ç¡®çš„JSONï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–æ–‡æœ¬ã€‚"
                    continue
                else:
                    print(f"â— AIè¿”å›çš„JSONæ ¼å¼æ— æ•ˆï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
                    print(result_text)
                    return {
                        'positive_tags': [],
                        'negative_tags': [],
                        'is_important': False,
                        'summary': content[:50] + ('...' if len(content) > 50 else '')
                    }
        except Exception as e:
            if retries < max_retries:
                retries += 1
                print(f"â— æ–°é—»å†…å®¹åˆ†æå¤±è´¥ (ç¬¬{retries}æ¬¡å°è¯•): {e}ï¼Œè¿›è¡Œé‡è¯•...")
                # çŸ­æš‚ç­‰å¾…åé‡è¯•
                time.sleep(1)
                continue
            else:
                print(f"â— æ–°é—»å†…å®¹åˆ†æå¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries}): {e}")
                # è¿”å›é»˜è®¤å€¼
                return {
                    'positive_tags': [],
                    'negative_tags': [],
                    'is_important': False,
                    'summary': content[:50] + ('...' if len(content) > 50 else '')
                }

def fetch_and_process(category: str = "hk_us"):
    ts = int(time.time())
    sign = generate_sign(ts, category)

    url = "https://www.cls.cn/v1/roll/get_roll_list"
    params = {
        "app": "CailianpressWeb",
        "category": category,
        "last_time": ts,
        "lastTime": ts,
        "os": "web",
        "refresh_type": "1",
        "rn": "20",
        "sv": "8.4.6",
        "sign": sign
    }
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
        "Referer": "https://www.cls.cn/telegraph"
    }

    # ç®€åŒ–è¯·æ±‚å‚æ•°æ—¥å¿—
    log_verbose(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] è·å–{category}ç±»åˆ«æ–°é—»...")
    
    result = {
        "success": True,
        "new_count": 0,
        "duplicate_count": 0,
        "error": None,
        "timestamp": datetime.now().isoformat()
    }

    try:
        resp = requests.get(url, headers=headers, params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()

        if data.get("errno") != 0:
            log_verbose(f"âŒ æ¥å£è¿”å›é”™è¯¯ï¼š{data.get('msg')}")
            return

        raw_list = data["data"]["roll_data"]
        raw_list = raw_list[:5]  # é™åˆ¶ä¸º20æ¡æ–°é—»

        # ç¡®å®šå½“å‰æ–°é—»ç±»åˆ«
        code = "cn" if category == "watch" else category
        
        # é¢„å¤„ç†ï¼šè®¡ç®—å†…å®¹å“ˆå¸Œå¹¶æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        items = []
        content_hashes = []
        for news in raw_list:
            content = news.get("content", "").strip()
            m = re.match(r"^ã€([^ã€‘]+)ã€‘(.*)", content)
            cleaned = m.group(2).strip() if m else content
            hash_value = content_hash(cleaned)
            content_hashes.append(hash_value)
            items.append((news, cleaned, hash_value))
        
        # æ‰¹é‡æ£€æŸ¥å“ªäº›å†…å®¹å“ˆå¸Œå·²å­˜åœ¨
        existing_hashes = check_content_hashes(content_hashes)
        
        # ç®€åŒ–å“ˆå¸Œæ£€æŸ¥æ—¥å¿—
        duplicate_by_hash = len(existing_hashes)
        if duplicate_by_hash > 0:
            log_verbose(f"å“ˆå¸Œæ£€æŸ¥: {duplicate_by_hash}/{len(raw_list)}æ¡æ–°é—»å·²å­˜åœ¨")
        
        # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„æ–°é—»
        items_to_process = [(news, cleaned) for news, cleaned, hash_value in items if hash_value not in existing_hashes]
        
        if not items_to_process:
            log_verbose("æ‰€æœ‰æ–°é—»å‡å·²å­˜åœ¨ï¼Œæ— éœ€å¤„ç†")
            result["duplicate_count"] = len(raw_list)
            return result
        
        log_verbose(f"å¤„ç† {len(items_to_process)} æ¡æ–°é—»...")
        
        # ä»…è·å–ç›¸åŒç±»åˆ«çš„æœ€è¿‘æ–°é—»ç”¨äºæ¯”è¾ƒ
        window_size = int(os.getenv("WINDOW_SIZE", 5))  # ä»ç¯å¢ƒå˜é‡è¯»å–çª—å£å¤§å°ï¼Œé»˜è®¤ä¸º5
        print(f"ğŸ” æ­£åœ¨åŠ è½½ {code} ç±»åˆ«çš„æœ€è¿‘ {window_size} æ¡æ–°é—»")
        
        existing_news = get_recent_news_with_embeddings(EMBEDDING_MODEL, code, limit=window_size)
        
        # æå–å·²æœ‰çš„åµŒå…¥å‘é‡
        existing_embeddings = []
        news_without_embeddings = []
        
        for news in existing_news:
            if news['embedding'] is not None:
                existing_embeddings.append((news['id'], news['embedding']))
            else:
                news_without_embeddings.append(news)
        
        print(f"ğŸ“Š å·²æ‰¾åˆ° {len(existing_embeddings)} æ¡å¸¦åµŒå…¥çš„æ–°é—»å’Œ {len(news_without_embeddings)} æ¡æ— åµŒå…¥çš„æ–°é—»")
        
        # ä¸ºæ²¡æœ‰åµŒå…¥çš„æ–°é—»é¡ºåºè®¡ç®—åµŒå…¥
        if news_without_embeddings:
            print(f"ğŸ”„ æ­£åœ¨ä¸º {len(news_without_embeddings)} æ¡æ–°é—»é¡ºåºè®¡ç®—åµŒå…¥")
            
            embeddings_to_save = []
            for news in news_without_embeddings:
                emb = create_embedding(news['content'], EMBEDDING_MODEL)
                if emb is not None:
                    existing_embeddings.append((news['id'], emb))
                    embeddings_to_save.append({
                        'news_id': news['id'],
                        'embedding_vector': emb,
                        'model_name': EMBEDDING_MODEL
                    })
            
            # ä¿å­˜æ–°è®¡ç®—çš„åµŒå…¥
            if embeddings_to_save:
                save_news_embeddings(embeddings_to_save)

        # é¡ºåºçˆ¬å–å†…å®¹å’Œç”Ÿæˆæ ‡é¢˜
        formatted_news = []
        embeddings_to_save = []
        new_count = 0
        duplicate_count = 0
        
        for idx, (news, cleaned) in enumerate(items_to_process, 1):
            # çˆ¬å–å†…å®¹
            link = extract_news_link(news.get("shareurl", ""))
            if link:
                summary, full_content = crawl_content(link)
                if full_content:
                    cleaned = full_content
            
            # ç”Ÿæˆæ ‡é¢˜
            title, cleaned_content = create_title(cleaned)
            
            # è®¡ç®—åµŒå…¥
            print(f"ğŸ”„ æ­£åœ¨ä¸ºç¬¬ {idx} æ¡æ–°é—»è®¡ç®—åµŒå…¥")
            emb = create_embedding(cleaned_content, EMBEDDING_MODEL)
            
            if emb is None:
                continue

            # æ£€æŸ¥æ˜¯å¦ä¸æœ€è¿‘çš„æ–°é—»é‡å¤
            is_duplicate = False
            for news_id, old_emb in existing_embeddings:
                if cosine_sim(emb, old_emb) >= SIM_THRESHOLD:
                    print(f"ğŸ”„ å‘ç°é‡å¤æ–°é—» (ID: {news_id}): {title}")
                    print(f"   ğŸ“ é“¾æ¥: {link}")
                    # æ‰¾åˆ°å¯¹åº”æ–°é—»å¯¹è±¡
                    old_news = next((n for n in existing_news if n['id'] == news_id), None)
                    if old_news:
                        # æ›´æ–°æ—§æ–°é—»
                        old_news["title"] = title
                        old_news["content"] = cleaned_content
                        old_news["ctime"] = format_ts(news.get("ctime", 0))
                        old_news["link"] = link
                        formatted_news.append(old_news)
                        
                        # æ›´æ–°åµŒå…¥
                        embeddings_to_save.append({
                            'news_id': news_id,
                            'embedding_vector': emb,
                            'model_name': EMBEDDING_MODEL
                        })
                    is_duplicate = True
                    duplicate_count += 1
                    break

            if not is_duplicate:
                formatted_news.append({
                    "ctime": format_ts(news.get("ctime", 0)),
                    "title": title,
                    "content": cleaned_content,
                    "link": link,
                    "code": code
                })
                print(f"ğŸ“Œ æ–°å¢ ç¬¬{idx}æ¡ | ğŸ•’ {format_ts(news.get('ctime', 0))}")
                print(f"   ğŸ“° {title}")
                print(f"   ğŸ“ {link}")
                new_count += 1

        print(f"ğŸ“Š å¤„ç†ç»“æœ: æ–°å¢ {new_count} æ¡æ–°é—», æ›´æ–° {duplicate_count} æ¡é‡å¤æ–°é—»")
        
        # ä¿å­˜æ–°é—»å†…å®¹å¹¶è·å–ç»“æœï¼ˆåŒ…å«æ–°é—»IDï¼‰
        saved_results = save_news(formatted_news)
        
        if saved_results:
            # ä¸ºæ–°å¢å’Œæ›´æ–°çš„æ–°é—»å‡†å¤‡åµŒå…¥æ•°æ®
            news_embeddings = []
            
            # å¤„ç†æ›´æ–°çš„æ–°é—»åµŒå…¥
            for data in embeddings_to_save:
                # æ£€æŸ¥æ­¤è®°å½•æ˜¯å¦è¢«è·³è¿‡
                news_id = data['news_id']
                if news_id in saved_results and saved_results[news_id].get('skipped'):
                    continue
                news_embeddings.append(data)
            
            # å¤„ç†æ–°å¢æ–°é—»çš„åµŒå…¥
            for news_item in formatted_news:
                for news_id, info in saved_results.items():
                    if info.get('is_new') and info['content'] == news_item['content'] and not info.get('skipped') and not info.get('existing'):
                        # ä¸ºæ–°æ–°é—»é‡æ–°è®¡ç®—åµŒå…¥
                        emb = create_embedding(news_item['content'], EMBEDDING_MODEL)
                        if emb is not None:
                            news_embeddings.append({
                                'news_id': news_id,
                                'embedding_vector': emb,
                                'model_name': EMBEDDING_MODEL
                            })
                        break
            
            # ä¿å­˜æ‰€æœ‰åµŒå…¥ï¼ˆæ–°å¢å’Œæ›´æ–°ï¼‰
            if news_embeddings:
                print(f"ğŸ”„ æ­£åœ¨ä¿å­˜ {len(news_embeddings)} æ¡æ–°é—»åµŒå…¥")
                save_news_embeddings(news_embeddings)
                
            # ä¸ºæ–°å¢çš„æ–°é—»ç”Ÿæˆå¹¶ä¿å­˜æ ‡ç­¾
            news_tags = []
            
            for news_id, info in saved_results.items():
                if info.get('is_new'):  # åªä¸ºæ–°å¢çš„æ–°é—»ç”Ÿæˆæ ‡ç­¾
                    # æ‰¾åˆ°å¯¹åº”çš„æ–°é—»å†…å®¹å’Œæ ‡é¢˜
                    for news_item in formatted_news:
                        if news_item['content'] == info['content']:
                            # åˆ†ææ–°é—»å†…å®¹
                            print(f"ğŸ·ï¸ æ­£åœ¨åˆ†ææ–°é—» {news_id} çš„å†…å®¹ä»¥ç”Ÿæˆæ ‡ç­¾...")
                            tag_info = analyze_news_content(news_item['content'], news_item['title'])
                            
                            # æ·»åŠ æ–°é—»ID
                            tag_info['news_id'] = news_id
                            news_tags.append(tag_info)
                            break

            # ä¿å­˜æ ‡ç­¾ä¿¡æ¯
            if news_tags:
                print(f"ğŸ”– æ­£åœ¨ä¸º {len(news_tags)} æ¡æ–°é—»ä¿å­˜æ ‡ç­¾å’Œæ‘˜è¦")
                save_news_tags(news_tags)
        
        # æ›´æ–°è¿”å›å€¼ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
        result["new_count"] = new_count
        result["duplicate_count"] = duplicate_count
        
        return result

    except Exception as e:
        print(f"â—è¯·æ±‚å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()
        result["success"] = False
        result["error"] = str(e)
        return result

def fetch_tops():
    """è·å–å¤´æ¡æ–°é—»"""
    base_url = "https://www.cls.cn/v3/depth/home/assembled/1000"
    params = {
        "app": "CailianpressWeb",
        "os": "web",
        "sv": "8.4.6",
    }

    # å¤´æ¡APIä½¿ç”¨å›ºå®šç­¾å
    params["sign"] = "9f8797a1f4de66c2370f7a03990d2737"

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
        "Referer": "https://www.cls.cn/"
    }

    result = {
        "success": True,
        "new_count": 0,
        "duplicate_count": 0,
        "error": None,
        "timestamp": datetime.now().isoformat()
    }

    try:
        resp = requests.get(base_url, headers=headers, params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()

        if data.get("errno") != 0:
            print(f"âŒ å¤´æ¡æ¥å£è¿”å›é”™è¯¯ï¼š{data.get('msg')}")
            result["success"] = False
            result["error"] = data.get("msg")
            return result

        articles = data["data"].get("top_article", [])
        print(f"âœ… è·å– {len(articles)} æ¡å¤´æ¡æ–°é—»ï¼š\n")

        formatted_news_list = []
        
        # é¡ºåºå¤„ç†æ¯ä¸ªé“¾æ¥
        for i, article in enumerate(articles, 1):
            schema = article.get("schema", "")
            link = extract_news_link(schema)
            
            # æå–æ ‡é¢˜å’Œå†…å®¹
            title = article.get("title", "").strip()
            content = article.get("brief", "").strip()
            
            # é¡ºåºçˆ¬å–å†…å®¹
            summary, full_content = None, None
            if link:
                summary, full_content = crawl_content(link)
                if full_content:
                    content = full_content
        
            formatted_news_list.append({
                "ctime": format_ts(article["ctime"]),
                "title": title,
                "content": content,
                "link": link,
                "code": "top",  # å¤´æ¡æ–°é—»ä½¿ç”¨"top"ç±»åˆ«
                "_crawled": bool(full_content),  # æ ‡è®°æ˜¯å¦å·²çˆ¬å–
                "_summary": summary  # ä¿å­˜æ‘˜è¦ä¿¡æ¯
            })
            print(f"ğŸ“Œ {i}. {title}")
            print(f"   ğŸ“ {link}")
            if summary:
                print(f"   ğŸ“ {summary[:50]}...")

        # è®¡ç®—å†…å®¹å“ˆå¸Œä»¥æ£€æŸ¥é‡å¤
        content_hashes = [content_hash(news["content"]) for news in formatted_news_list]
        existing_hashes = check_content_hashes(content_hashes)
        
        # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„æ–°é—»
        news_to_process = []
        duplicate_count = 0
        
        for news, hash_value in zip(formatted_news_list, content_hashes):
            if hash_value not in existing_hashes:
                news_to_process.append(news)
            else:
                duplicate_count += 1
        
        if not news_to_process:
            print("æ‰€æœ‰å¤´æ¡æ–°é—»å·²å­˜åœ¨ï¼Œæ— éœ€ä¿å­˜")
            result["duplicate_count"] = duplicate_count
            return result
        
        print(f"â³ å¤„ç† {len(news_to_process)} æ¡å¤´æ¡æ–°é—»...")
        
        # è·å–"top"ç±»åˆ«çš„æœ€è¿‘æ–°é—»ç”¨äºæ¯”è¾ƒ
        window_size = int(os.getenv("WINDOW_SIZE", 5))
        existing_news = get_recent_news_with_embeddings(EMBEDDING_MODEL, "top", limit=window_size)
        
        # æå–å·²æœ‰çš„åµŒå…¥å‘é‡
        existing_embeddings = []
        for news in existing_news:
            if news['embedding'] is not None:
                existing_embeddings.append((news['id'], news['embedding']))
        
        if existing_embeddings:
            print(f"ğŸ“Š ä¸ºå¤´æ¡æ–°é—»è¿›è¡ŒåµŒå…¥ç›¸ä¼¼åº¦æ£€æŸ¥ï¼Œå·²åŠ è½½ {len(existing_embeddings)} æ¡ç°æœ‰åµŒå…¥")
            
            # ä¸ºæ–°æ–°é—»é¡ºåºè®¡ç®—åµŒå…¥
            unique_news = []
            sim_duplicates = 0
            
            for news in news_to_process:
                emb = create_embedding(news['content'], EMBEDDING_MODEL)
                if emb is None:
                    # å¦‚æœåµŒå…¥å¤±è´¥ï¼Œè¿˜æ˜¯ä¿ç•™è¿™æ¡æ–°é—»
                    unique_news.append(news)
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰æ–°é—»ç›¸ä¼¼
                is_similar = False
                for news_id, old_emb in existing_embeddings:
                    if cosine_sim(emb, old_emb) >= SIM_THRESHOLD:
                        print(f"ğŸ”„ å‘ç°ç›¸ä¼¼å¤´æ¡æ–°é—» (ID: {news_id}): {news['title']}")
                        is_similar = True
                        sim_duplicates += 1
                        break
                
                if not is_similar:
                    unique_news.append(news)
            
            duplicate_count += sim_duplicates
            print(f"ğŸ“Š åµŒå…¥ç›¸ä¼¼åº¦æ£€æŸ¥: å‘ç° {sim_duplicates} æ¡ç›¸ä¼¼æ–°é—»")
        else:
            # å¦‚æœæ²¡æœ‰å¯æ¯”è¾ƒçš„åµŒå…¥ï¼Œåˆ™æ‰€æœ‰é€šè¿‡å“ˆå¸Œæ£€æŸ¥çš„æ–°é—»éƒ½è§†ä¸ºå”¯ä¸€
            unique_news = news_to_process
        
        # ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“
        if unique_news:
            saved_results = save_news(unique_news)
            
            # ä¸ºæ–°ä¿å­˜çš„æ–°é—»åˆ›å»ºåµŒå…¥
            if saved_results:
                news_embeddings = []
                news_tags = []
                
                # ä¸ºæ¯æ¡æ–°ä¿å­˜çš„æ–°é—»è®¡ç®—åµŒå…¥å’Œæ ‡ç­¾
                for news_id, info in saved_results.items():
                    if info.get('is_new'):
                        # è®¡ç®—å¹¶ä¿å­˜åµŒå…¥
                        emb = create_embedding(info['content'], EMBEDDING_MODEL)
                        if emb is not None:
                            news_embeddings.append({
                                'news_id': news_id,
                                'embedding_vector': emb,
                                'model_name': EMBEDDING_MODEL
                            })
                        
                        # è®¡ç®—å¹¶ä¿å­˜æ ‡ç­¾
                        # æ‰¾åˆ°åŸå§‹æ–°é—»æ•°æ®ä»¥è·å–æ ‡é¢˜
                        news_item = next((news for news in unique_news if news['content'] == info['content']), None)
                        if news_item:
                            print(f"ğŸ·ï¸ æ­£åœ¨åˆ†æå¤´æ¡æ–°é—» {news_id} çš„å†…å®¹ä»¥ç”Ÿæˆæ ‡ç­¾...")
                            tag_info = analyze_news_content(info['content'], news_item.get('title'))
                            
                            # æŸ¥æ‰¾å·²ç»çˆ¬å–çš„æ‘˜è¦
                            summary = news_item.get('_summary')
                            if summary:
                                tag_info['summary'] = summary
                            
                            tag_info['news_id'] = news_id
                            news_tags.append(tag_info)
                
                # ä¿å­˜æ‰€æœ‰åµŒå…¥
                if news_embeddings:
                    print(f"ğŸ”„ æ­£åœ¨ä¿å­˜ {len(news_embeddings)} æ¡å¤´æ¡æ–°é—»åµŒå…¥")
                    save_news_embeddings(news_embeddings)
                
                # ä¿å­˜æ‰€æœ‰æ ‡ç­¾
                if news_tags:
                    print(f"ğŸ”– æ­£åœ¨ä¸º {len(news_tags)} æ¡å¤´æ¡æ–°é—»ä¿å­˜æ ‡ç­¾å’Œæ‘˜è¦")
                    save_news_tags(news_tags)
            
            result["new_count"] = len(unique_news)
            
        else:
            print("æ‰€æœ‰å¤´æ¡æ–°é—»ç»è¿‡åµŒå…¥ç›¸ä¼¼åº¦æ£€æŸ¥åå‡å·²å­˜åœ¨ï¼Œæ— éœ€ä¿å­˜")
        
        result["duplicate_count"] = duplicate_count
        return result
    
    except Exception as e:
        print(f"â— å¤´æ¡è¯·æ±‚å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()
        result["success"] = False
        result["error"] = str(e)
        return result

def process_category(category_info):
    """å¤„ç†å•ä¸ªæ–°é—»ç±»åˆ«çš„åŒ…è£…å‡½æ•°"""
    cat_name, cat_type = category_info
    try:
        print(f"ğŸ”„ å¼€å§‹å¤„ç† {cat_name}")
        if cat_type == 'top':
            result = fetch_tops()
        else:
            result = fetch_and_process(cat_type)
        
        if result and result.get('success'):
            new_count = result.get('new_count', 0)
            dup_count = result.get('duplicate_count', 0)
            print(f"âœ… {cat_name} å®Œæˆ: æ–°å¢{new_count}æ¡ï¼Œå·²å­˜åœ¨{dup_count}æ¡")
        else:
            print(f"âŒ {cat_name} å¤±è´¥")
            
        return (cat_name, cat_type, result)
    except Exception as e:
        print(f"âŒ {cat_name} å‡ºé”™: {str(e)}")
        return (cat_name, cat_type, e)

def main():
    """ä¸»å‡½æ•°ï¼šå¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ–°é—»è·å–æµç¨‹"""
    # é¦–å…ˆåˆå§‹åŒ–æ•°æ®åº“ï¼Œç¡®ä¿æ‰€æœ‰è¡¨éƒ½å­˜åœ¨
    print("åˆå§‹åŒ–æ•°æ®åº“...")
    initialize_database()
    
    # å®šä¹‰å¯ç”¨çš„æ–°é—»ç±»åˆ«
    CATEGORIES = {
        'hk_us': 'æ¸¯ç¾è‚¡æ–°é—»',
        'watch': 'å›½å†…Aè‚¡æ–°é—»',
        'top': 'è´¢è”ç¤¾å¤´æ¡'
    }
    
    # æ›´æ•´æ´çš„å¼€å§‹ä¿¡æ¯
    print("\n" + "="*30)
    print("å¼€å§‹å¹¶è¡Œè·å–è´¢ç»æ–°é—»")
    print("="*30)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = datetime.now()
    
    # å‡†å¤‡å¹¶è¡Œå¤„ç†çš„å‚æ•°
    category_tasks = [(name, cat_type) for cat_type, name in CATEGORIES.items()]
    
    # ä½¿ç”¨ThreadPoolExecutorå¹¶è¡Œå¤„ç†
    category_results = {}
    with ThreadPoolExecutor(max_workers=3) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_category = {
            executor.submit(process_category, task): task 
            for task in category_tasks
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_category):
            try:
                cat_name, cat_type, result = future.result()
                category_results[cat_type] = (cat_name, result)
            except Exception as e:
                task = future_to_category[future]
                cat_name, cat_type = task
                category_results[cat_type] = (cat_name, e)
    
    # å¤„ç†ç»“æœ
    total_new = 0
    total_duplicate = 0
    errors = []
    
    # æ•´æ´åœ°æ˜¾ç¤ºç»“æœæ±‡æ€»
    print("\n" + "-"*30)
    print("è·å–ç»“æœ:")
    
    for cat_type in CATEGORIES.keys():
        if cat_type in category_results:
            cat_name, result = category_results[cat_type]
            
            if isinstance(result, Exception):
                print(f"â€¢ {cat_name}: âŒ é”™è¯¯ ({str(result)[:50]}...)")
                errors.append(f"{cat_type}: {result}")
                continue
                
            if result and result.get('success'):
                new_count = result.get('new_count', 0)
                dup_count = result.get('duplicate_count', 0)
                total_new += new_count
                total_duplicate += dup_count
                status = "âœ…" if new_count > 0 else "ğŸ”„"
                print(f"â€¢ {cat_name}: {status} æ–°å¢{new_count}æ¡ï¼Œå·²å­˜åœ¨{dup_count}æ¡")
            else:
                print(f"â€¢ {cat_name}: âŒ å¤±è´¥")
                if result:
                    errors.append(f"{cat_type}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        else:
            print(f"â€¢ {CATEGORIES[cat_type]}: âŒ æœªå¤„ç†")
    
    # è¾“å‡ºæ€»ä½“ç»“æœï¼Œæ›´ç®€æ´æ˜äº†
    print("\n" + "="*30)
    print(f"âœ… å¹¶è¡Œä»»åŠ¡å®Œæˆ (è€—æ—¶: {(datetime.now() - start_time).total_seconds():.1f}ç§’)")
    print(f"ğŸ“Š æ€»è®¡: æ–°å¢{total_new}æ¡ï¼Œå·²å­˜åœ¨{total_duplicate}æ¡")
    if errors:
        print(f"âŒ é”™è¯¯: {len(errors)}ä¸ª")
    print("="*30)
    
    return 0 if not errors else 1

if __name__ == "__main__":
    """è„šæœ¬å…¥å£ç‚¹"""
    import sys
    sys.exit(main())